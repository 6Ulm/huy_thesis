\chapter[Annexes]{Annexes}

\localtableofcontents

% \addcontentsline{toc}{chapter}{Annexes}

\chaptermark{Annexes}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proof in Chapter 3}

For later convenience, we define the function
$\vert \xi_1 - \xi_2 \vert^p: (X_1^s \times X_2^s) \times (X_1^f \times X_2^f) \to \bbR_{\geq 0}$ by
\begin{equation*}
    \vert \xi_1 - \xi_2 \vert^p \big((x_1^s, x_2^s), (x_1^f, x_2^f)\big) :=
    \vert \xi_1(x_1^s, x_1^f) - \xi_2(x_2^s, x_2^f) \vert^p,
\end{equation*}
and write the objective function of generalised COOT as
\begin{equation*}
    F_{\lambda}(\pi^s, \pi^f) = \iint |\xi_1 - \xi_2|^p \mathrm d\pi^s \mathrm d \pi^f
    + \sum_{k=1}^2\lambda_k D_k(\pi^s_{\#k} \otimes \pi^f_{\#k} \vert \mu^s_k \otimes \mu^f_k).
\end{equation*}
The generalized COOT now reads compactly as
\begin{equation} \label{eq:ucoot_copy}
%   \ucoot_{\lambda}(\cX_1, \cX_2) :=
  \inf_{\substack{\pi^s \in \cM^+(X_1^s \times X_2^s) \\
  \pi^f \in \cM^+(X_1^f \times X_2^f) \\ m(\pi^s) = m(\pi^f)}} F_{\lambda}(\pi^s, \pi^f)
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proofs related to the properties of UCOOT}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{claim}
  When $D_k = \iota_{=}$ and $\mu_k^s, \mu_k^f$ are probability measures, for $k=1,2$,
  then we recover COOT from generalized COOT.
\end{claim}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
  Under the above assumptions, the generalized COOT problem becomes
  \begin{equation*}
    \begin{split}
      \inf_{\substack{\pi^s \in \cM^+(X_1^s \times X_2^s) \\
      \pi^f \in \cM^+(X_1^f \times X_2^f)}}
      &\iint |\xi_1 - \xi_2|^p \mathrm d\pi^s \mathrm d \pi^f \\
      \text{subject to } &\pi^s_{\#1} \otimes \pi^f_{\#1} = \mu_1^s \otimes \mu_1^f \text{ (C1) } \\
      & \pi^s_{\#2} \otimes \pi^f_{\#2} = \mu_2^s \otimes \mu_2^f \text{ (C2) } \\
      & m(\pi^s) = m(\pi^f) \text{ (C3) }.
    \end{split}
  \end{equation*}
  As $m(\pi) = m(\pi_{\#1}) = m(\pi_{\#2})$,
  for any measure $\pi$, and $\mu_k^s, \mu_k^f$ are probability measures, for $k=1,2$,
  one has $m(\pi^s) m(\pi^f) = 1$, thus $m(\pi^s) = m(\pi^f) = 1$.
  Now, the constraint C1 implies that
  $\int_{X_1^s} \mathrm d\pi^s_{\#1} \mathrm d \pi^f_{\#1}
  = \int_{X_1^s} \mathrm d\mu_1^s \mathrm d\mu_1^f$. Thus, $\pi^f_{\#1} = \mu_1^f$.
  Similarly, we have $\pi^s_{\#k} = \mu_k^s$ and $\pi^f_{\#k} = \mu_k^f$, for any $k=1,2$.
  We conclude that $\pi^f \in U(\mu_1^f, \mu_2^f)$ and $\pi^s \in U(\mu_1^s, \mu_2^s)$,
  and we obtain the COOT problem.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proposition}
    \label{eq:ucoot_existence_copy}
  (Existence of minimizer) Denote
  $\cS := (X_1^s \times X_2^s) \times (X_1^f \times X_2^f)$.
  The problem \ref{eq:ucoot_copy} admits a minimizer if at least one of
  the following conditions hold:
  \begin{enumerate}
    \item The entropy functions $\phi_1$ and $\phi_2$ are superlinear, \textit{i.e}.
    $(\phi_1)'_{\infty} = (\phi_2)'_{\infty} = \infty$.
    \item The function $\vert c_X - c_Y \vert^p$ has compact sublevels in $\cS$ and
    $\inf_{\cS} \vert \xi_1 - \xi_2 \vert^p + \lambda_1 (\phi_1)'_{\infty} + \lambda_2 (\phi_2)'_{\infty} > 0$.
  \end{enumerate}
\end{proposition}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
  We adapt the proof of Theorem 3.3 in \citep{Liero18} and of Proposition 3 in \citep{Sejourne20}.
  For convenience, we write $\mu_1 = \mu_1^s \otimes \mu_1^f$ and
  $\mu_2 = \mu_2^s \otimes \mu_2^f$. For each pair $(\pi^s, \pi^f)$, denote
  $\pi = \pi^s \otimes \pi^f$.
  It can be shown that
  $\pi_{\# k} := (P_{X_k^s \times X_k^f})_{\#} \pi
  = (P_{X_k^s})_{\#} \pi^s \otimes (P_{X_k^f})_{\#} \pi^f =
  \pi^s_{\# k} \otimes \pi^f_{\# k}$, for $k=1,2$. Indeed, for any function
  $\phi \in \mathcal C_b(X_k^s \times X_k^f)$, we have
    \begin{equation*}
      \begin{split}
        \int_{X_k^s \times X_k^f} \phi \;
        \mathrm d (P_{X_k^s X_k^f})_{\#} \pi
        &= \int_{\cS} (\phi \circ P_{X_k^s X_k^f}) \; \mathrm d\pi \\
        &= \int_{\cS} \phi(x_k^s, x_k^f)
        \; \mathrm d \pi^s(x_1^s, x_2^s) \; \mathrm d \pi^f(x_1^f, x_2^f) \\
        &= \int_{X_k^s \times X_k^f} \phi \; \mathrm d \pi^s_{\# k} \;
        \mathrm d \pi^f_{\# k}.
      \end{split}
    \end{equation*}
  Thus, the problem \ref{eq:ucoot_copy} can be rewritten as
  \begin{equation*}
    \ucoot_{\lambda}(\cX_1, \cX_2) =
    \inf_{\pi \in E_{uco}} \int_{\cS} \vert \xi_1 - \xi_2 \vert^p
    \mathrm d\pi + \sum_{k=1,2} \lambda_k D_{\phi_k}(\pi_{\# k} \vert \mu_k),
  \end{equation*}
  where
  \begin{equation*}
    E_{uco} = \{ \pi \in \cM^+(\cS) \vert \pi = \pi^s \otimes \pi^f,
    \pi^s \in \cM^+(X_1^s \times X_2^s),
    \pi^f \in \cM^+(X_1^f \times X_2^f) \}.
  \end{equation*}
  Define
  \begin{equation*}
    L(\pi):= \int_{\cS} \vert \xi_1 - \xi_2 \vert^p \mathrm d \pi +
    \sum_{k=1,2} \lambda_k D_{\phi_k}(\pi_{\# k} \vert \mu_k).
  \end{equation*}
  By Jensen's inequality, we have
  \begin{equation*}
    \begin{split}
      L(\pi) &\geq m(\pi) \inf_{\cS} \vert \xi_1 - \xi_2 \vert^p +
      \sum_{k=1,2} \lambda_k m(\mu_k) \phi_k \Big( \frac{m(\pi_{\# k})}{m(\mu_k)} \Big) \\
      &= m(\pi) \bigg[ \inf_{\cS} \vert \xi_1 - \xi_2 \vert^p +
      \sum_{k=1,2} \lambda_k \frac{m(\mu_k)}{m(\pi)} \phi_k
      \Big( \frac{m(\pi)}{m(\mu_k)} \Big) \bigg],
    \end{split}
  \end{equation*}
  where, in the last equality, we use the relation $m(\pi) = m(\pi_{\# k})$, for $k=1,2$.
  It follows from the assumption that $L$ is coercive. So, $L(\pi) \to \infty$
  when $m(\pi) \to \infty$.

  Clearly $\inf_{E_{uco}} L < \infty$ because
  $L\big( (\mu_1^s \otimes \mu_2^s) \otimes (\mu_1^f \otimes \mu_2^f) \big) < \infty$.
  Let $(\pi_n)_n \subset {E_{uco}}$ be a minimizing sequence, meaning that
  $L(\pi_n) \to \inf_{E_{uco}} L$.
  Such sequence is necessarily bounded (otherwise, there exists a subsequence $(\pi_{n_k})_{n_k}$
  with $m(\pi_{n_k}) \to \infty$ and the coercivity of $L$ implies $L(\pi_{n_k}) \to \infty$,
  which is absurd). Suppose $m(\pi_{n}) \leq M$, for some $M > 0$. By Tychonoff's theorem,
  as $X_k^s$ and $X_k^f$ are compact spaces,
  so is the product space $\cS$. Thus, by Banach-Alaoglu theorem,
  the ball $B_M = \{ \pi \in \cM^+(\cS): m(\pi) \leq M \}$
  is weakly compact in $\cM^+(\cS)$.

  Consider the set $\overline{E}_{uco} = E_{uco} \cap B_M$, then clearly
  $(\pi_n)_n \subset \overline{E}_{uco}$. We will show that
  there exists a converging subsequence of $(\pi_n)_n$, whose limit is in $\overline{E}_{uco}$,
  thus $\overline{E}_{uco}$ is weakly compact. Indeed, by definition of $E_{uco}$,
  there exist two sequences $(\pi_n^s)_n$ and $(\pi_n^f)_n$ such that
  $\pi_n = \pi_n^s \otimes \pi_n^f$.
  We can assume furthermore that $m(\pi_n^s) = m(\pi_n^f) = \sqrt{m(\pi_n)} \leq \sqrt M$.
  As $m(\pi_n^s)$ and $m(\pi_n^f)$ are bounded, by reapplying Banach-Alaoglu theorem,
  one can extract two converging subsequences (after reindexing)
  $\pi_n^s \rightharpoonup \pi^s \in \cM^+(X_1^s \times X_2^s)$ and
  $\pi_n^f \rightharpoonup \pi^f \in \cM^+(X_1^f \times X_2^f)$,
  with $m(\pi^s) = m(\pi^f) \leq \sqrt{M}$.
  An immediate extension of Theorem 2.8 in \citep{Billingsley99} to the convergence of
  the products of bounded positive measures implies
  $\pi_n^s \otimes \pi_n^f \rightharpoonup \pi^s \otimes \pi^f \in \overline{E}_{uco}$.

  Now, the lower semicontinuity of $L$ implies that $\inf_{E_{uco}} L \geq L(\pi^s \otimes \pi^f)$,
  thus $L(\pi^s \otimes \pi^f) = \inf_{E_{uco}} L$ and $(\pi^s, \pi^f)$
  is a solution of the problem \ref{eq:ucoot_copy}.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{claim}
  Suppose that $\cX_1$ and $\cX_2$ are two finite sample-feature spaces such that
  $(X^s_1, X^s_2)$ and $(X^f_1, X^f_2)$
  have the same cardinality and are equipped with the uniform measures
  $\mu_1^s = \mu_2^s$, $\mu_1^f = \mu_2^f$. Then $\ucoot_{\lambda}(\cX_1, \cX_2) = 0$
  if and only if there exist perfect alignments between rows (samples) and between
  columns (features) of the interaction matrices $\xi_1$ and $\xi_2$.
\end{claim}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
  Without loss of generality, we can assume that $\mu_k^s$ and $\mu_k^f$ are
  discrete uniform probability distributions, for $k=1,2$. By Proposition 1 in \citep{Redko20},
  under the assumptions on $\cX_1$ and $\cX_2$, we have
  $\coot(\cX_1, \cX_2) = 0$ if and only if there exist perfect alignments
  between rows (samples) and between columns (features) of the interaction matrices $\xi_1$ and
  $\xi_2$. So, it is enough to prove that $\ucoot_{\lambda}(\cX_1, \cX_2) = 0$
  if and only if $\coot(\cX_1, \cX_2) = 0$.

  Let $(\pi^s, \pi^f)$ be a pair of equal-mass couplings such that
  $\ucoot_{\lambda}(\cX_1, \cX_2) = 0$. It follows that
  $\pi^s_{\#k} \otimes \pi^f_{\#k} = \mu_k^s \otimes \mu_k^f$, for $k=1,2$. Consequently,
  $m(\pi^s) m(\pi^f) = m(\mu_1^s) m(\mu_1^f) = 1$, so $m(\pi^s) = m(\pi^f) = 1$. Now, we have
  $\int_{X_k^s} \mathrm d \pi^s_{\#k} \; \mathrm d \pi^f_{\#k}
  = \int_{X_k^s} \mathrm d\mu_k^s \; \mathrm d\mu_k^f$, or equivalently,
  $\pi^f_{\#k} = \mu_k^f$. Similarly, $\pi^s_{\#k} = \mu_k^s$, meaning that
  $\pi^s \in U(\mu_1^s, \mu_2^s)$ and $\pi^f \in U(\mu_1^f, \mu_2^f)$. Thus,
  $\coot(\cX_1, \cX_2) = \ucoot_{\lambda}(\cX_1, \cX_2) = 0$.

  For the other direction, suppose that $\coot(\cX_1, \cX_2) = 0$.
  Let $(\pi^s, \pi^f)$ be a pair of couplings such that $\coot(\cX_1, \cX_2) = 0$.
  As $\pi^s \in U(\mu_1^s, \mu_2^s)$ and $\pi^f \in U(\mu_1^f, \mu_2^f)$, one has
  $\coot(\cX_1, \cX_2) = F_{\lambda}(\pi^s, \pi^f) \geq
  \ucoot_{\lambda}(\cX_1, \cX_2) \geq 0$,
  for every $\lambda_1, \lambda_2 > 0$. So, $\ucoot_{\lambda}(\cX_1, \cX_2) = 0$.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%ù
\subsection{Robustness of UCOOT and sensitivity of COOT}

First, we recall our assumptions.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{assumption}
\label{assump:robust_copy}
Consider two sample-feature spaces
$\bbX_k = ((X^s_k, \mu^s_k), (X^f_k, \mu^f_k), \xi_k)$, for $k=1,2$.
Let $\varepsilon^s$ (resp. $\varepsilon^f$) be a probability measure with compact support $O^s$
(resp. $O^f$). For $a \in \{s, f\}$, define the noisy distribution
$\widetilde{\mu}^a = \alpha_a \mu^a + (1-\alpha_a) \varepsilon^a$, where $\alpha_a \in [0,1]$.
We assume that $\xi_1$ is defined on
$(X^s_1 \cup O^s) \times (X^f_1 \cup O^f)$ and that $\xi_1, \xi_2$
are continuous on their supports. We denote the contaminated sample-feature space by
$\widetilde{\cX_1} = ((X^s_1 \cup O^s, \widetilde{\mu}^s_1),
(\cX^f_1 \cup O^f, \widetilde{\mu}^f_1), \xi_1)$. Finally,
we define some useful minimal and maximal costs:
  \[
  \begin{cases}
  \Delta_{0} =& \min_{
  \substack{
       x_1^s \in O^s, x_1^f \in O^f  \\
       x_2^s \in \cX_2^s, x_2^f \in \cX_2^f
  }}\quad |\xi_1(x_1^s, x_1^f) - \xi_2(x_2^s, x_2^f)|^p \\
  \Delta_{\infty} =& \max_{
  \substack{
  x_1^s \in \cX_1^s \cup O^s, x_1^f \in \cX_1^f \cup O^f \\
  x_2^s \in \cX_2^s, x_2^f \in \cX_2^f
  }} \quad|\xi_1(x_1^s, x_1^f) - \xi_2(x_2^s, x_2^f)|^p \enspace.
  \end{cases}
\]
\end{assumption}
For convenience, we write $C = \vert \xi_1 - \xi_2 \vert^p$ and
$\widetilde{\cS} := (X^s_1 \cup O^s) \times X_2^s \times (X^f_1 \cup O^f) \times X_2^f$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proposition}[COOT is sensitive to outliers]
Consider $\widetilde{\cX_1}, \cX_2$ as defined in \Cref{assump:robust_copy}.
Then:
 \label{prop:coot-not-robust_copy}
\begin{equation*}
    \coot(\widetilde{\cX_1}, \cX_2) \geq (1 - \alpha_s)(1-\alpha_f) \Delta_0.
\end{equation*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{proposition}
\begin{proof}
Consider a pair of feasible alignments $(\pi^s, \pi^f)$. Since $C$ is non-negative, taking the COOT integral over a smaller set leads to the lower bound:
% \coot(\cX_1, \widetilde{\cX_2}) &= \min_{\substack{\pi^s \in \cU(\mu_1^s, \widetilde{\mu_2^s}) \\ \pi^f \in \cU(\mu_1^f, \widetilde{\mu_2^f})}}
\begin{equation*}
    \begin{split}
        \int_{\widetilde{\cS}} C \mathrm d\pi^s\mathrm d\pi^f
        &\geq \int_{O^s \times \cX_2^s \times O^f \times \cX_2^f} C \mathrm d\pi^s\mathrm d\pi^f \\
          &\geq  \Delta_0 \int_{O^s \times \cX_2^s \times O^f \times \cX_2^f}  \mathrm d\pi^s\mathrm d\pi^f \\
          &= \Delta_0 \int_{O^s\times O^f}  \mathrm d\pi^s_{\#1} \mathrm d\pi^f_{\#1} \\
          &\geq (1 - \alpha_s)(1 -\alpha_f)\Delta_0,
    \end{split}
\end{equation*}
where the last inequality follows from the marginal constraints.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{theorem}[UCOOT is robust to outliers]
  \label{thm:ucoot_robust_copy}
  Consider two sample-feature spaces $\widetilde{\cX_1}, \cX_2$ as defined in
  \Cref{assump:robust_copy}. Let
  $\delta = 2(\lambda_1 + \lambda_2)(1 - \alpha_s\alpha_f)$ and
  $K = M + \frac{1}{M}\ucoot(\cX_1, \cX_2) +\delta$, where
  $M= m(\pi^s) = m(\pi^f)$ is the transported mass between clean data. Then:
     \begin{equation*} %\label{eq:ucoot-robust}
      \begin{split}
        \ucoot(\widetilde{\cX_1}, \cX_2)
        \leq \alpha_s \alpha_f \ucoot(\cX_1, \cX_2) +
        \delta M \left[ 1 - \exp \left( {- \frac{\Delta_{\infty}(1+M) + K}{\delta M}} \right) \right].
      \end{split}\vspace{-10mm}
    \end{equation*}
  \end{theorem}
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  To get the exponential bound of this theorem, we use the following lemma.
  \begin{lemma}
  \label{slem:bound}
  Let $\varphi: t \in (0, 1] \mapsto t\log(t) - t + 1$ and
  $f_{a, b}: t \in (0, 1] \mapsto t \to at + b \varphi(t)$ for some $a, b > 0$.
  Then:
  \begin{equation*}
      \min_{t \in (0, 1]} f_{a, b}(t) = b(1 - e^{-a/b}) = f_{a, b}(e^{-\frac{a}{b}}).
  \end{equation*}
  \end{lemma}
  \begin{proof}
    Since $f_{a,b}$ is convex, cancelling the gradient is sufficient for optimality.
    The solution follows immediately.
  \end{proof}
  \begin{proof}
    The proof uses the same core idea of \citep{Fatras21} but is slightly more technical
    for two reasons: (1) we consider arbitrary outlier distributions instead of simple Diracs;
    (2) we consider sample-feature outliers which requires more technical derivations.

    The idea of proof is as follows. First, we construct sample and feature couplings
    from the solution of "clean" UCOOT and the reference measures. Then, they are used to
    upper bound the "noisy" UCOOT. By manipulating this bound, the "clean" UCOOT term will appear.
    A variable $t \in (0,1)$ is also introduced in the fabricated couplings.
    The upper bound becomes a function of $t$ and can be optimized to obtain the final bound.

    Now, we prove Theorem 2.

    \paragraph{Fabricating sample and feature couplings.}
    Given the equal-mass solution $(\pi^s, \pi^f)$ of the UCOOT problem,
    with $m(\pi^s) = m(\pi^f) = M$, consider, for $t \in (0,1)$, a pair of
    sub-optimal transport plans:
    \begin{align}
      &\widetilde{\pi}^s = \alpha_s \pi^s + t (1-\alpha_s) \varepsilon_s \otimes \mu^s_2\\
      &\widetilde{\pi}^f = \alpha_f \pi^f + t (1-\alpha_f) \varepsilon_f \otimes \mu^f_2.
    \end{align}
    Then, for $a\in \{s, f\}$, it holds:
    \begin{itemize}
      \item $\widetilde{\pi}^a_{\#1} = \alpha_k \pi^a_{\#1} + t (1 - \alpha_a) \varepsilon_a$,
      \item $\widetilde{\pi}^a_{\#2} = \alpha_k \pi^a_{\#2} + t (1 - \alpha_a) \mu^a_2$,
      \item $m(\widetilde{\mu}^a_1) = 1$ and $m(\widetilde{\pi}^a) = \alpha_a M + (1-\alpha_a) t$.
    \end{itemize}
    \paragraph{Establishing and manipulating the upper bound.}
    Denote $q = (1 - \alpha_s)(1 - \alpha_f), s = \alpha_s (1-\alpha_f) + \alpha_f (1 - \alpha_s)$
    and recall that on $\widetilde{\cS}$, the cost $C$ is upper bounded by
    $\Delta_{\infty} = \max_{\widetilde{\cS}}|\xi_1 - \xi_2|^p$.
    First we upper bound the transportation cost:
    \begin{equation*}
      \label{seq:cost-split}
      \begin{split}
        &\int_{\widetilde{\cS}} C \; \mathrm d\widetilde{\pi}^s \; \mathrm d\widetilde{\pi}^f \\
        &= \alpha_s\alpha_f\int_{\widetilde{\cS}} C \; \mathrm d\pi^s \; d\pi^f +
        t \sum_{k \neq i} (1-\alpha_i) \alpha_k \int_{\widetilde{\cS}} C \;
        \mathrm d \varepsilon_i \; \mathrm d\mu^i_2  \; \mathrm d\pi^k +
        q t^2 \int_{\widetilde{\cS}} C \; \mathrm d \varepsilon_s \; \mathrm d\mu_2^s \;
        \mathrm d\varepsilon_f \; \mathrm d\mu^f_2 \\
        &\leq \alpha_s\alpha_f \int_{\cS} C \; \mathrm d\pi^s \;\mathrm d\pi^f +
        \Delta_{\infty}(Ms + q)t\enspace,
      \end{split}
    \end{equation*}
   since $t^2 \leq t$.

  Second, we turn to the KL marginal discrepancies. We would like to extract the KL terms
  involving only the clean transport plans from the contaminated ones.
  We first detail both joint KL divergences for the source measure indexed by 1.
  The same holds for the target measure:
    \begin{equation}
    \label{seq:kl-split}
    \begin{split}
      &\kl(\widetilde{\pi}^s_{\#1} \otimes \widetilde{\pi}^f_{\#1} \vert \widetilde{\mu}^s_1 \otimes \widetilde{\mu}^f_1) =
      \sum_{k \neq i} m(\widetilde{\pi}^i) \kl(\widetilde{\pi}^k_{\#1} \vert \widetilde{\mu}^k_1) +
      \prod_{k} \big( m(\widetilde{\pi}^k) - 1 \big)\\
      &
      \kl(\pi^s_{\#1} \otimes \pi^f_{\#1} \vert \mu^s_1 \otimes \mu^f_1) =
    M \sum_k \kl(\pi^k_{\#1} \vert \mu^k_1) + (M-1)^2.
      \end{split}
    \end{equation}
    Now we upper bound each smaller KL term using the joint convexity of the KL divergence:
    \begin{equation*}
      \begin{split}
        \kl(\widetilde{\pi}^k_{\#1} \vert \widetilde{\mu}^k_1) &\leq
        \alpha_k \kl(\pi^k_{\#1} \vert \mu^k_1) + (1 - \alpha_k) \kl(t \varepsilon_k \vert \varepsilon_k) \\
        &= \alpha_k \kl(\pi^k_{\#1} \vert \mu^k_1) + (1 - \alpha_k) \varphi(t),
      \end{split}
    \end{equation*}
    where $\varphi(t) = t \log t - t + 1$, for $t > 0$. Thus, for $k\neq i$:
    \begin{equation*}
      \begin{split}
        &m(\widetilde{\pi}^i) \kl(\widetilde{\pi}^k_{\#1} \vert \widetilde{\mu}^k_1)
        \leq m(\widetilde{\pi}^i) \alpha_k \kl(\pi^k_{\#1} \vert \mu^k_1)
        + m(\widetilde{\pi}^i) (1 - \alpha_k) \varphi(t) \\
        &= \alpha_i\alpha_k M \kl(\pi^k_{\#1} \vert \mu^k_1)
        + t (1-\alpha_i) \alpha_k \kl(\pi^k_{\#1} \vert \mu^k_1)
        + \alpha_i(1-\alpha_k) M \varphi(t) + t q\varphi(t).
      \end{split}
    \end{equation*}
    Summing over $f$ and $s$, we obtain:
    \begin{equation*}
      \begin{split}
        &\sum_{k \neq i} m(\widetilde{\pi}^i) \kl(\widetilde{\pi}^k_{\#1} \vert \widetilde{\mu}^k_1) \\
        &\leq \alpha_s\alpha_f M \sum_k \kl(\pi^k_{\#1} \vert \mu^k_1) +
        t \sum_{k \neq i} (1-\alpha_i) \alpha_k \kl(\pi^k_{\#1} \vert \mu^k_1)
        + M s \varphi(t)+ 2q t \varphi(t) \\
        &\leq (\alpha_s\alpha_f + \frac{t s}{M})\left(\kl(\pi^s_{\#1} \otimes \pi^f_{\#1}
        \vert \mu^s_1 \otimes \mu^f_1) - (1-M)^2\right) + M s \varphi(t)+ 2q t \varphi(t).
        \end{split}
    \end{equation*}
     where, in the last bound,  we used the second equation of \eqref{seq:kl-split}
     and the fact that $\alpha_s(1-\alpha_f) \leq s$ and  $\alpha_f(1-\alpha_s) \leq s$.
     The product of masses of \eqref{seq:kl-split} can be written:
    \begin{equation*}
      \begin{split}
        \prod_{k} \big( m(\widetilde{\pi}^k) - 1 \big) &= \prod_k \big( \alpha_k(M-1)
        + (1-\alpha_k)(t-1) \big) \\
        &= \alpha_s\alpha_f(1-M)^2 + s(1-M)(1-t) + q(1-t)^2.
      \end{split}
    \end{equation*}
    Thus, combining these upper bounds for the source measure:
    \begin{equation*}
      \begin{split}
        \kl(\widetilde{\pi}^s_{\#1} \otimes \widetilde{\pi}^f_{\#1} \vert \widetilde{\mu}^s_1 \otimes \widetilde{\mu}^f_1)
        &\leq \alpha_s\alpha_f \kl(\pi^s_{\#1} \otimes \pi^f_{\#1} \vert \mu^s_1 \otimes \mu^f_1) \\
        &+ \frac{ts}{M}\left(\kl(\pi^s_{\#1} \otimes \pi^f_{\#1} \vert \mu^s_1 \otimes \mu^f_1) - (1-M)^2\right) \\
        &+ \big[  sM \varphi(t)+ 2q t \varphi(t) + s(1-M)(1-t) + q(1-t)^2 \big],
      \end{split}
    \end{equation*}
    and similarly, for the target measure:
    \begin{equation*}
      \begin{split}
        \kl(\widetilde{\pi}^s_{\#2} \otimes \widetilde{\pi}^f_{\#2} \vert \mu^s_2 \otimes \mu^f_2)
        &\leq \alpha_s\alpha_f \kl(\pi^s_{\#2} \otimes \pi^f_{\#2} \vert \mu^s_2 \otimes \mu^f_2) \\
        &+ \frac{ts}{M}\left(\kl(\pi^s_{\#2} \otimes \pi^f_{\#2} \vert \mu^s_2 \otimes \mu^f_2)
        - (1-M)^2\right) \\
       &+ \big[ sM \varphi(t)+ 2q t \varphi(t) + s(1-M)(1-t) + q(1-t)^2 \big].
      \end{split}
    \end{equation*}
    Then, for every $0 < t \leq 1$, by summing all bounds:
    \begin{equation*}
      \begin{split}
        \ucoot(\widetilde{\cX_1}, \cX_2) &\leq \alpha_s\alpha_f \ucoot(\cX_1, \cX_2) +
        \Delta_{\infty}(Ms + q)t \\
        &+ \frac{ts}{M}(\ucoot(\cX_1, \cX_2) - (\lambda_1 + \lambda_2)(1-M)^2) \\
        &+ (\lambda_1 + \lambda_2) \big[ s M \varphi(t) + 2q t \varphi(t)
        + s(1-M)(1-t) + q(1-t)^2 \big].
      \end{split}
    \end{equation*}
    \paragraph{Minimizing the upper bound with respect to $t$.}
    To obtain the exponential bound, we would like have an upper bound of the form
    $at + b\varphi(t)$, so that lemma \ref{slem:bound} applies.
    Knowing that $1 \leq 2(t + \varphi(t))$ for any $t \in [0, 1]$:
    Let's first isolate the quantity that is not of this form:
    We have:
    \begin{equation*}
      \begin{split}
        2q t \varphi(t) + s(1 - M) + q(t-1)^2 &= 2qt^2\log(t) - 2qt^2 + 2qt + s(1-M) + qt^2 -2qt + q \\
        &=  2qt^2\log(t) - qt^2 + s(1-M) + q \\
        &= q\varphi(t^2) + s(1-M) \leq q + s(1-M) \\
        &\leq 2(q + s(1-M)) (t + \varphi(t)) \\
        &= 2(1 -\alpha_s\alpha_f - sM) (t + \varphi(t)).
      \end{split}
    \end{equation*}
    The new full bound is given by:
    \begin{equation*}
        \ucoot(\widetilde{\cX_1}, \cX_2) \leq \alpha_s\alpha_t \ucoot(\cX_1, \cX_2) + A' t + B'\varphi(t),
    \end{equation*}
    where
    \begin{equation*}
        \begin{split}
            A' &= \Delta_{\infty}(Ms + q) + s(M-1) + \frac{s}{M}\ucoot(\cX_1, \cX_2) - \frac{s}{M}(\lambda_1 + \lambda_2)(1-M)^2 \\
            &+ 2(\lambda_1 + \lambda_2) (1-\alpha_s\alpha_f - sM) \\
            & \leq \Delta_{\infty}(M + 1) + M + \frac{1}{M}\ucoot(\cX_1, \cX_2) + 2(\lambda_1 + \lambda_2) (1-\alpha_s\alpha_f) = A \\
            B' &= 2sM(\lambda_1 + \lambda_2) (1-\alpha_s\alpha_f) \leq 2M(\lambda_1 + \lambda_2) (1-\alpha_s\alpha_f) = B.
        \end{split}
    \end{equation*}
    In both inequalities, we use the fact that $s \leq 1 - \alpha_s \alpha_f \leq 1$.
    Using Lemma \ref{slem:bound}, we obtain
    \begin{equation*}
        \ucoot(\widetilde{\cX_1}, \cX_2)
        \leq \alpha_s\alpha_f \ucoot(\cX_1, \cX_2) + B \left[ 1 - \exp{ \left(- \frac{A}{B} \right) }\right].
    \end{equation*}
    The upper bound of \Cref{thm:ucoot_robust_copy} then follows.
  \end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Numerical aspects}
We claim that, in the discrete setting, by taking $\varepsilon$ sufficiently small in the entropic UCOOT problem, we can obtain a solution ``close'' to the non-entropic case. We formalize this claim and prove it in the following result.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{claim}
  \label{convergence_minimiser_unbalanced}
  Let $(\pi_{\varepsilon}^s, \pi_{\varepsilon}^f)$ be an equal-mass solution of the problem
  $\ucoot_{\lambda, \varepsilon}(\cX_1, \cX_2)$. Denote $\mu^s = \mu_1^s \otimes \mu_2^s$ and $\mu^f = \mu_1^f \otimes \mu_2^f$.
  \begin{enumerate}
    \item When $\varepsilon \to \infty, \pi_{\varepsilon}^s \rightharpoonup \sqrt{\frac{m(\mu^f)}{m(\mu^s)}} \mu^s$
    and $\pi_{\varepsilon}^f  \rightharpoonup \sqrt{\frac{m(\mu^s)}{m(\mu^f)}} \mu^f$.

    \item When $\varepsilon \to 0$, if the spaces $X_k^s$ and $X_k^f$ are finite,
    for $k=1,2$, then $\ucoot_{\lambda, \varepsilon}(\cX_1, \cX_2) \to \ucoot_{\lambda}(\cX_1, \cX_2)$ and
    any cluster point $\widehat{\pi}^s \otimes \widehat{\pi}^f$ of the sequence
    $(\pi_{\varepsilon}^s \otimes \pi_{\varepsilon}^f)_{\varepsilon}$ will induce an equal-mass
    solution $(\widehat{\pi}^s, \widehat{\pi}^f)$ of the problem
    $\ucoot_{\lambda}(\cX_1, \cX_2)$. Furthermore,
    \begin{equation*}
      \kl(\widehat{\pi}^s \otimes \widehat{\pi}^f | \mu^s \otimes \mu^f) =
      \min_{(\pi^s, \pi^f)} \kl(\pi^s \otimes \pi^f \vert \mu^s \otimes \mu^f),
    \end{equation*}
    where the infimum is taken over all equal-mass solutions of $\ucoot_{\lambda}(\cX_1, \cX_2)$.
  \end{enumerate}
\end{claim}
%%%%%%%%%%%%%%%%%%%
\begin{proof}
  Denote $\pi_{\varepsilon} = \pi_{\varepsilon}^s \otimes \pi_{\varepsilon}^f$.
  \begin{enumerate}
    \item When $\varepsilon \to \infty$: the sub-optimality of $\left( \sqrt{\frac{m(\mu^f)}{m(\mu^s)}} \mu^s, \sqrt{\frac{m(\mu^s)}{m(\mu^f)}} \mu^f \right)$ implies
    \begin{equation*}
      \begin{split}
        \varepsilon \kl(\pi_{\varepsilon} \vert \mu^s \otimes \mu^f)
        &\leq F_{\lambda}(\pi_{\varepsilon}^s, \pi_{\varepsilon}^f) +
        \varepsilon \kl(\pi_{\varepsilon} \vert \mu^s \otimes \mu^f) \\
        &\leq F_{\lambda} \left( \sqrt{\frac{m(\mu^f)}{m(\mu^s)}} \mu^s, \sqrt{\frac{m(\mu^s)}{m(\mu^f)}} \mu^f \right) +
        \varepsilon \kl( \mu^s \otimes \mu^f \vert \mu^s \otimes \mu^f) \\
        &= \iint \vert \xi_1 - \xi_2 \vert^p \mathrm d\mu^s \mathrm d\mu^f.
      \end{split}
    \end{equation*}
    Thus,
    \begin{equation*}
      0 \leq \kl(\pi_{\varepsilon} \vert \mu^s \otimes \mu^f)
      \leq \frac{1}{\varepsilon} \iint \vert \xi_1 - \xi_2 \vert^p
      \mathrm d\mu^s \mathrm d\mu^f \to 0,
    \end{equation*}
    whenever $\varepsilon \to \infty$. We deduce that $\kl(\pi_{\varepsilon} \vert \mu^s \otimes \mu^f)$,
    thus $\pi_{\varepsilon} \rightharpoonup \mu^s \otimes \mu^f$. The conclusion then follows.

    \item Let $(\pi_*^s, \pi_*^f)$ be a solution of
    $\ucoot_{\lambda}(\cX_1, \cX_2)$.
    The optimality of $(\pi_{\varepsilon}^s, \pi_{\varepsilon}^f)$ implies
    \begin{equation*}
      \begin{split}
        \ucoot_{\lambda}(\cX_1, \cX_2)
      &\leq \ucoot_{\lambda}(\cX_1, \cX_2) +
      \varepsilon \kl(\pi_*^s \otimes \pi_*^f \vert \mu^s \otimes \mu^f).
      \end{split}
    \end{equation*}
    Thus, when $\varepsilon \to 0$, one has
    $\ucoot_{\lambda, \varepsilon}(\cX_1, \cX_2) \to
    \ucoot_{\lambda}(\cX_1, \cX_2)$. Now, for every $\varepsilon > 0$,
    \begin{equation*}
      \begin{split}
        \langle C, \mu^s \otimes \mu^f \rangle &=
        F_{\lambda} \left( \sqrt{\frac{m(\mu^f)}{m(\mu^s)}} \mu^s, \sqrt{\frac{m(\mu^s)}{m(\mu^f)}} \mu^f \right) +
        \varepsilon \kl( \mu^s \otimes \mu^f \vert \mu^s \otimes \mu^f ) \\
        &\geq F_{\lambda}(\pi_{\varepsilon}^s, \pi_{\varepsilon}^f) +
        \varepsilon \kl(\pi_{\varepsilon}^s \otimes \pi_{\varepsilon}^f \vert \mu^s \otimes \mu^f) \\
        &\geq F_{\lambda}(\pi_{\varepsilon}^s, \pi_{\varepsilon}^f).
      \end{split}
    \end{equation*}
    On the other hand, following the same proof in \Cref{eq:ucoot_existence_copy}, we can show that if
    $m(\pi_{\varepsilon}) \to \infty$, then $F_{\lambda}(\pi_{\varepsilon}^s, \pi_{\varepsilon}^f) \to \infty$, which
    contradicts the above inequality. So, there exists $M > 0$ such that $m(\pi_{\varepsilon}) \leq M$,
    for every $\varepsilon > 0$.

    The set $\widetilde{E}_{uco} = \{\pi \in \cM^+(\cS): m(\pi) \leq M\} \cap E_{uco}$ is clearly compact,
    thus from the sequence of minimisers $(\pi_{\varepsilon})_{\varepsilon} \subset \widetilde{E}_{uco}$
    (i.e. $\pi_{\varepsilon} = \pi_{\varepsilon}^s \otimes \pi_{\varepsilon}^f$), we can extract a
    converging subsequence $(\pi_{\varepsilon_n})_{\varepsilon_n}$ such that
    $\pi_{\varepsilon_n} \to \widehat{\pi} = \widehat{\pi}^s \otimes \widehat{\pi}^f \in \widetilde{E}_{uco}$,
    with $m(\widehat{\pi}^s) = m(\widehat{\pi}^f)$.
    The continuity of the divergences implies that,
    $F_{\lambda, \varepsilon}(\pi_{\varepsilon_n}^s, \pi_{\varepsilon_n}^f) \to
    F_{\lambda}(\widehat{\pi}^s, \widehat{\pi}^f)$, when $\varepsilon \to 0$. We deduce that
    $\ucoot_{\lambda}(\cX_1, \cX_2) = F_{\lambda}(\widehat{\pi}^s, \widehat{\pi}^f)$,
    or equivalently $(\widehat{\pi}^s, \widehat{\pi}^f)$
    is a solution of $\ucoot_{\lambda}(\cX_1, \cX_2)$. Moreover, we have
    \begin{equation} \label{unbalanced_max_ent}
      \begin{split}
        0 &\leq F_{\lambda}(\pi_{\varepsilon_n}^s, \pi_{\varepsilon_n}^f) - F_{\lambda}(\pi_*^s, \pi_*^f) \\
      &\leq \varepsilon_n \Big( \kl(\pi_*^s \otimes \pi_*^f \vert \mu^s \otimes \mu^f) -
      \kl(\pi_{\varepsilon_n}^s \otimes \pi_{\varepsilon_n}^f \vert \mu^s \otimes \mu^f) \Big).
      \end{split}
    \end{equation}
    Dividing by $\varepsilon_n$ in \ref{unbalanced_max_ent} and let $\varepsilon_n \to 0$, we have
    \begin{equation*}
      \kl(\widehat{\pi}^s \otimes \widehat{\pi}^f \vert \mu^s \otimes \mu^f) \leq
      \kl(\pi_*^s \otimes \pi_*^f \vert \mu^s \otimes \mu^f).
    \end{equation*}
    and we deduce that
    \begin{equation*}
      \kl(\widehat{\pi}^s \otimes \widehat{\pi}^f \vert \mu^s \otimes \mu^f) =
      \min_{(\pi^s, \pi^f)} \kl(\pi^s \otimes \pi^f \vert \mu^s \otimes \mu^f),
    \end{equation*}
    where the infimum is taken over all solutions of $\ucoot_{\lambda}(\cX_1, \cX_2)$.
  \end{enumerate}
\end{proof}

\section{Proof of Chapter }

\begin{proof}[Proof of \Cref{prop:basic_prop}]
  The proof of this proposition can be adapted directly from \citep{Vayer19b}.
  For self-contained purpose, we give the proof here. Denote
  \begin{itemize}
      \item $(P^s_{\alpha}, P^v_{\alpha})$ the optimal sample and feature couplings for
      $\agw_{\alpha}(X, Y)$.

      \item $(P^s_0, P^v_0)$ the optimal sample and feature couplings for $\coot(X, Y)$.

      \item $P^s_1$ the optimal sample coupling for $\gw(X, Y)$.
  \end{itemize}
  Due to the suboptimality of $P^s_{\alpha}$ for GW and $(P^s_1, P^v_0)$ for AGW, we have
  \begin{align}
      \alpha \langle L(D_X, D_Y) \otimes P^s_1, P^s_1 \rangle
      &\leq \alpha \langle L(D_X, D_Y) \otimes P^s_{\alpha}, P^s_{\alpha} \rangle
      + (1 - \alpha) \langle L(X, Y) \otimes P^v_{\alpha}, P^s_{\alpha} \rangle \\
      &\leq \alpha \langle L(D_X, D_Y) \otimes P^s_1, P^s_1 \rangle
      + (1 - \alpha) \langle L(X, Y) \otimes P^v_0, P^s_1 \rangle,
  \end{align}
  or equivalently
  \begin{align}
      \alpha \gw(X, Y) \leq \agw_{\alpha}(X, Y) \leq \alpha \gw(X, Y)
      + (1 - \alpha) \langle L(X, Y) \otimes P^v_0, P^s_1 \rangle.
  \end{align}
  Similarly, we have
  \begin{align}
      (1 - \alpha) \coot(X, Y) \leq \agw_{\alpha}(X, Y) \leq (1 - \alpha) \coot(X, Y)
      + \alpha \langle L(D_X, D_Y) \otimes P^s_0, P^s_0 \rangle.
  \end{align}
  The interpolation property then follows by the sandwich theorem.

  Regarding the relaxed triangle inequality, given three triplets
  $(X, \mu_{sx}, \mu_{fx}), (Y, \mu_{sy}, \mu_{fy})$ and $(Z, \mu_{sz}, \mu_{fz})$,
  let $(\pi^{XY}, \gamma^{XY}), (\pi^{YZ}, \gamma^{YZ})$ and $(\pi^{XZ}, \gamma^{XZ})$
  be solutions of the problems $\agw_{\alpha}(X, Y), \agw_{\alpha}(Y, Z)$ and
  $\agw_{\alpha}(X, Z)$, respectively. Denote
  $P = \pi^{XY} \diag \left( \frac{1}{\mu_{sy}} \right) \pi^{YZ}$ and
  $Q = \gamma^{XY} \diag\left( \frac{1}{\mu_{fy}} \right) \gamma^{YZ}$. Then,
  it is not difficult to see that $P \in U(\mu_{sx}, \mu_{sz})$ and
  $Q \in U(\mu_{fx}, \mu_{fz})$. The suboptimality of $(P,Q)$ implies that
  \begin{align}
      &\frac{\agw_{\alpha}(X, Z)}{2} \\
      &\leq \alpha \sum_{i,j,k,l} \frac{|D_X(i,j) - D_Z(k,l)|^2}{2} P_{i,k} P_{j, l}
      + (1 - \alpha) \sum_{i,j,k,l} \frac{|X_{i,j} - Z_{k,l}|^2}{2} P_{i,k} Q_{j,l} \\
      &= \alpha \sum_{i,j,k,l} \frac{|D_X(i,j) - D_Z(k,l)|^2}{2}
      \left( \sum_e \frac{\pi^{XY}_{i,e} \pi^{YZ}_{e,k}}{(\mu_{sy})_e} \right)
      \left(\sum_o \frac{\pi^{XY}_{j,o} \pi^{YZ}_{o,l}}{(\mu_{sy})_o} \right) \\
      &+ (1 - \alpha) \sum_{i,j,k,l} \frac{|X_{i,j} - Z_{k,l}|^2}{2}
      \left(\sum_e \frac{\pi^{XY}_{i,e} \pi^{YZ}_{e,k}}{(\mu_{sy})_e} \right)
      \left( \sum_o \frac{\gamma^{XY}_{j,o} \gamma^{YZ}_{o,l}}{(\mu_{fy})_o} \right) \\
      &\leq \alpha \sum_{i,j,k,l,e,o} |D_X(i,j) - D_Y(e,o)|^2
      \frac{\pi^{XY}_{i,e} \pi^{YZ}_{e,k}}{(\mu_{sy})_e}
      \frac{\pi^{XY}_{j,o} \pi^{YZ}_{o,l}}{(\mu_{sy})_o}
      + (1 - \alpha) \sum_{i,j,k,l,e,o} |X_{i,j} - Y_{e,o}|^2
      \frac{\pi^{XY}_{i,e} \pi^{YZ}_{e,k}}{(\mu_{sy})_e}
      \frac{\gamma^{XY}_{j,o} \gamma^{YZ}_{o,l}}{(\mu_{fy})_o} \\
      &+ \alpha \sum_{i,j,k,l,e,o} |D_Y(e, o) - D_Z(k,l)|^2
      \frac{\pi^{XY}_{i,e} \pi^{YZ}_{e,k}}{(\mu_{sy})_e}
      \frac{\pi^{XY}_{j,o} \pi^{YZ}_{o,l}}{(\mu_{sy})_o}
      + (1 - \alpha) \sum_{i,j,k,l,e,o} |Y_{e, o} - Z_{k,l}|^2
      \frac{\pi^{XY}_{i,e} \pi^{YZ}_{e,k}}{(\mu_{sy})_e}
      \frac{\gamma^{XY}_{j,o} \gamma^{YZ}_{o,l}}{(\mu_{fy})_o} \\
      &= \alpha \sum_{i,j,e,o} |D_X(i,j) - D_Y(e,o)|^2 \pi^{XY}_{i,e} \pi^{XY}_{j,o}
      + (1 - \alpha) \sum_{i,j,e,o} |X_{i,j} - Y_{e,o}|^2 \pi^{XY}_{i,e} \gamma^{XY}_{j,o} \\
      &+ \alpha \sum_{k, l, e, o} |D_Y(e, o) - D_Z(k,l)|^2 \pi^{YZ}_{e,k} \pi^{YZ}_{o,l}
      + (1 - \alpha) \sum_{k,l,e,o} |Y_{e, o} - Z_{k,l}|^2 \pi^{YZ}_{e,k} \gamma^{YZ}_{o,l} \\
      &= \agw_{\alpha}(X, Y) + \agw_{\alpha}(Y, Z).
  \end{align}
  where the second inequality follows from the inequality: $(x + y)^2 \leq 2(x^2 + y^2)$.
\end{proof}


\begin{proof}[Proof of \Cref{corr:hermitian}]
  First, let us recall the Schwartz-Zippel lemma. Denote $F(x_1, ..., x_n)$ a multivariate polynomial.
  Its total degree is the maximum of the sums of the powers of the variables in any monomial.
  The Schwartz-Zippel lemma states that: let $F(x_1, ..., x_n)$ be a nonzero multivariate polynomial
  of total degree $d$ and $S$ be a finite subset of $\bbR$. Denote
  $Z_S := \{ (x_1, ..., x_n) \in S^n : F(x_1, ..., x_n) = 0 \}$ the zero set of $F$ on $S^n$.
  Then $| Z_S | \leq d |S|^{n-1}$.

  Note that, the set of Hermitian matrices of size $n$ forms a finite-dimensional real vector space.
  In particular, it is isomorphic to the Euclidean space $\bbR^{n^2}$.
  Denote $I$ set of Hermitian matrices of size $n$ with repeated eigenvalues.
  It is enough to show that $I$ has measure zero. We have $I \simeq E$,
  for some $E \subset \bbR^{n^2}$. Since $I$ is closed (see page 56 in \citep{Tao12}),
  it is measurable, by Proposition 4 in \citep{Stein05}. If $I$ does not have zero measure,
  then the intersection $E \cap [0, 1]^{n^2}$ has positive measure $p > 0$. If,
  for each $i \in [n^2]$, we sample $m$ i.i.d coordinates uniformly in $[0, 1]$,
  then we have $m^{n^2}$ points uniformly distributed in $[0, 1]^{n^2}$. So,
  the expected number of points lying in $E$ is $pm^{n^2}$.

  On the other hand, recall that a (Hermitian) matrix has repeated eigenvalues if and only if
  the discriminant of its characteristic polynomial is zero. Moreover,
  the discriminant of the characteristic polynomial is a polynomial in $n^2$ entries of the matrix.
  Thus, the measure of $I$ (or, equivalently $E$) is the measure of the set of values of these
  $n^2$ variables which make a certain polynomial of total degree $d$ vanish.
  By Schwartz-Zippel lemma, on average, there are at most $d m^{n^2-1}$ points in $E$.
  By choosing $m > d / p$, we obtain a contradiction. Thus $E$ (or equivalently $I$)
  must have zero measure.
\end{proof}


\begin{proof}[Proof of \Cref{thm:invariant}]
  Regarding the first claim, note that $Y = X Q$,
  where $Q$ is a permutation matrix corresponding to the permutation $\sigma_c$.
  Since $Y$ is obtained by swapping columns of $X$, it is easy to see that $\gw(X, Y) = 0$
  and the optimal plan between $X$ and $Y$ is $P^s = \frac{1}{n^2} \id_n$.
  Similarly, $\coot(X, Y) = 0$ and $P^s, P^v = \frac{1}{n}Q$ are the optimal sample,
  feature couplings, respectively. In other words,
  $\langle L(D_{X},D_{Y})\otimes P^s, P^s \rangle = 0$ and
  $\langle L(X,Y) \otimes P^v, P^s \rangle = 0$. We deduce that $\agw_{\alpha}(X, Y) = 0$.

  Now, if $\agw(X, Y) = 0$, then $\gw(X, Y) = \coot(X, Y) = 0$. In particular,
  $X$ and $Y$ must have the same shape, assume that $X, Y \in \bbR^{n \times d}$.
  As $\gw(X, Y) = 0$, there exists an isometry from $X \to Y$. Note that every isometry from
  $\bbR^d$ to $\bbR^d$ is a composition of at most $d+1$ reflections
  (see, for example, Corollary A.7 in \citep{Konrad}). So, $U \in \cO_d$
  exists such that $Y = X U$. As $\coot(X, Y) = 0$, there exist two permutations
  $\sigma_r$ and $\sigma_c$ such that $X_{i, j} = Y_{\sigma_r(i), \sigma_c(j)}$,
  or equivalently $P \in \cP_n, Q_1 \in \cP_d$ such that $Y = P X Q_1$.
  We deduce that $X U = P X Q_1$, or equivalently $X = P X Q$, for
  $Q = Q_1 U^T \in \cO_d$. From this equation, we now aim to deduce the property of $Q$.

  To do this, consider the singular value decomposition of $X$, \textit{\textit{i.e.}}
  $X = U \Sigma V^T$, where $U \in \bbR^{n \times d}$ s.t. $U^T U = I_d$,
  $V \in \cO_d$ and $\Sigma \in \bbR^{d \times d}$ is a diagonal matrix
  whose diagonal contains $d$ strictly decreasing singular values. Since $X = P X Q$,
  we have $U \Sigma V^T = (PU) \Sigma (V^T Q)$. For $i \in [d]$, let $u_i \in \bbR^n$
  and $v_i \in \bbR^d$ be columns of $U$ and $V$, respectively.
  As the singular values are positive and distinct, the columns are unique up to
  the sign change of both columns in $U$ and $V$. This means $u_i = \pm Pu_i$ and
  $v_i = \pm Q^T v_i$. In other words, $\pm 1$ are eigenvalues of $P$ and $Q^T$, and
  $u_i, v_i$ are their corresponding eigenvectors, respectively. Denote $D \in \bbR^d$
  any diagonal matrix whose diagonal contains only $\pm 1$, then $Q^T = V D V^{-1} = V D V^T = Q$.
  So, $Q$ is symmetric. \Cref{thm:invariant} then follows by observing that $U = Q^T Q_1$.
  \end{proof}


\subsubsection{Weak invariant to translation}
While enjoying the interpolation and metric properties, AGW does not inherit the invariance
to the translation of the GW distance. However, we find that it satisfies a relaxed version
of this invariant defined as follows.
\begin{definition}
    We call $D = \inf_{\pi \in U} F(\pi, X, Y)$, where $X, Y$ are input data and $U$
    is a set of feasible couplings and $F$ is a real-valued functional, an OT-based divergence.
    Then $D$ is weakly invariant to translation if for every $a, b \in \bbR$, we have
    $\inf_{\pi \in \Pi} F(\pi, X, Y) = C + \inf_{\pi \in \Pi} F(\pi, X + a, Y+b)$,
    for some constant $C$ depending on $a, b, X, Y$ and $\Pi$.
\end{definition}
Here, we denote the translation of $X$ as $X + a$, whose elements are of the form $X_{ij} + a$.
In other words, an OT-based divergence is weakly invariant to translation if only
the optimal transport plan is preserved under translation, but not necessarily the divergence itself.
In practice, we would argue that the ability to preserve the optimal plan under translation
is much more important than preserving the distance itself. In other words,
the translation only shifts the minimum but has no impact on the optimization procedure,
meaning that the minimizer remains unchanged. Now, we can show that
\begin{corollary}
\label{prop:coot_invariant}
    COOT is weakly invariant to translation.
\end{corollary}
\begin{proof}[Proof of \Cref{prop:coot_invariant}]
It is enough to show that, for any $c \in \bbR$, we have $\coot(X, Y + c) = \coot(X, Y) + C$,
for some constant $C$. Indeed, given $P^s \in U(\mu, \nu), P^v \in U(\mu', \nu')$,
for any $c \in \bbR$,
\begin{align}
    \sum_{ijkl} (X_{ik} - Y_{jl} - c)^2 P^s_{ij} P^v_{kl}
    &= \sum_{ijkl} (X_{ik} - Y_{jl})^2 P^s_{ij} P^v_{kl}
    - 2c \sum_{ijkl} (X_{ik} - Y_{jl}) P^s_{ij} P^v_{kl} + c^2
\end{align}
Now,
\begin{align}
    \sum_{ijkl} (X_{ik} - Y_{jl}) P^s_{ij} P^v_{kl}
    &= \sum_{ijkl} X_{ik} P^s_{ij} P^v_{kl} - \sum_{ijkl} Y_{jl} P^s_{ij} P^v_{kl} \\
    &= \sum_{ik} X_{ik} \left( \sum_j P^s_{ij} \right) \left( \sum_l P^v_{kl} \right)
    - \sum_{jl} Y_{jl} \left( \sum_i P^s_{ij} \right) \left( \sum_k P^v_{kl} \right) \\
    &= \sum_{ik} X_{ik} \mu_i \mu'_k - \sum_{jl} Y_{jl} \nu_j \nu'_l \\
    &= \mu ^T \X \mu' - \nu ^T Y \nu'.
\end{align}
So,
\begin{align}
    \coot(X, Y + c) = \coot(X, Y) - 2 c \left( \mu^T X \mu' - \nu^T Y \nu' \right) + c^2.
\end{align}
This implies that COOT is weakly invariant to translation.
\end{proof}

AGW inherits the weak invariant to translation from COOT.
\begin{proposition}
\label{prop:invariant}
AGW is weakly invariant to translation.
\end{proposition}
\begin{proof}[Proof of \Cref{prop:invariant}]
Note that the GW term in AGW remains unchanged by translation.
By adapting the proof of \Cref{prop:coot_invariant}, we obtain
\begin{align}
    \agw_{\alpha}(X, Y + c) = \agw_{\alpha}(X, Y) + (1 - \alpha) \left[ c^2
    - 2 c \left( \mu^T X \mu' - \nu^T Y \nu' \right) \right].
\end{align}
The result then follows.
\end{proof}