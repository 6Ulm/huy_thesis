\section{Appendix of Chapter 2}

\subsection{Proofs related to Unbalanced Optimal Transport} \label{appendix:subsec_uot}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}[Proof of \Cref{coro:uot_l2_mm}]
Following \citep{Chapel21}, we can write Problem \eqref{uot_l2} as
\begin{align}
    \min_{t \in \bbR^{mn}_{\geq 0}} \langle c, t \rangle + \frac{|| Mt - y ||_2^2}{2},
\end{align}
where $t = \vect(P), c = \vect(C)$ are the vectorizations of $P, C$, respectively. Here,
\begin{itemize}
    \item[$\bullet$] $M = (\rho_1^{1/2} M_r^T, \rho_2^{1/2} M_c^T, \varepsilon^{1/2} I_{mn})^T \in \bbR^{(m + n + mn) \times (mn)}$.

    \item[$\bullet$] $y = (\rho_1^{1/2} \mu^T, \rho_2^{1/2} \nu^T, \varepsilon^{1/2} \vect(\gamma)^T)^T \in \bbR^{m + n + mn}$.

    \item[$\bullet$] $M_r = \texttt{numpy.repeat(numpy.eye(n),m)} \in \bbR^{n \times mn}$.

    \item[$\bullet$] $M_c = [I_m, ..., I_m] = \texttt{numpy.tile(numpy.eye(m),n)} \in \bbR^{m \times mn}$.
\end{itemize}
See Appendix A in \citep{Chapel21} for more details of $M_r, M_c$.
Remark that if $A \in \bbR^{m \times n}$ and $B \in \bbR^{m \times p}$, then
\begin{align}
    (A, B) \begin{pmatrix}
        A^T \\
        B^T
    \end{pmatrix}
    = A A^T + B B^T.
\end{align}
Following Equation 23 in \citep{Chapel21}, for $i \in [mn]$,
\begin{align}
    t^{(k+1)}_i = t^{(k)}_i \frac{\max \big\{ 0, (M^T y)_i - c_i \big\}}{(M^T M t^{(k)})_i}.
\end{align}
Now, we convert the vector $t$ back to the transport plan. More precisely,
$\text{mat}(M^T y) = (\rho_1 \mu) \oplus (\rho_2 \nu) + \varepsilon \gamma$,
where matrization is the inverse operation of vectorization. Since,
$M^T M = \rho_1 M_r^T M_r + \rho_2 M_c^T M_c + \varepsilon I_{mn}$, we obtain
$\text{mat}(M^T M t^{(k)}) = (\rho_1 P_{\# 1}^{(k)}) \oplus (\rho_2 P_{\# 2}^{(k)})
+ \varepsilon P^{(k)} $. The result then follows.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}[Proof of \Cref{prop:uot_minimizer}]
We follow the same proof technique of Lemma 4 in \citep{Khiem20}.
Given a Bregman divergence $D_{\psi}$, for any $t \in \bbR$ such that $tp \in \dom(\psi)$,
we have
\begin{align}
  D_{\psi}(t p | q)
  &= \psi(tp) - \psi(q) - \langle \nabla \psi(q), tp - q \rangle \\
  &= \psi(tp) - \psi(q) - \Big[
    t \langle \nabla \psi(q), p - q \rangle + (t-1) \langle \nabla \psi(q), q \rangle
  \Big] \\
  &= \psi(tp) - \psi(q) + t \Big[ D_{\psi}(p | q) + \psi(q) - \psi(p) \Big]
  - (t-1) \langle \nabla \psi(q), q \rangle \\
  &= t D_{\psi}(p | q) + \big[ \psi(tp) - t \psi(p) \big]
  + (t-1) \big[ \psi(q) - \langle \nabla \psi(q), q \rangle \big] \\
  &= t D_{\psi}(p | q) + \big[ \psi(tp) - t \psi(p) \big] + (1 - t) G_{\psi}(q).
\end{align}
Denote $g$ the objective function of Problem \eqref{eq:uot_bregman}. We have,
\begin{align}
  g(tP) &= t g(P) +
  \underbrace{\Big( \sum_{k=1}^2 \rho_k \big[ \varphi_k(tP_{\# k}) - t \varphi_k(P_{\# k}) \big]
  + \varepsilon \big[ \varphi(tP) - t \varphi(P) \big] \Big)}_{A(t)} \\
  &+ (1 - t) \underbrace{\big[ \rho_k G_{\varphi_k}(\mu_k) + \varepsilon G_{\varphi}(\gamma) \big]}_{B} \\
    &= t g(P) + A(t) + (1 - t) B.
\end{align}
Since $P^*$ is a minimizer, we must have $1 \in \argmin\limits_{t \in \bbR: tP^* \in \cD} g(tP^*)$
or equivalently $\frac{\partial g(tP^*)}{\partial t} \Big |_{t = 1} = 0$. So,
\begin{align}
  g(P^*) &= B - \frac{\partial A(t)}{\partial t} \bigg|_{t=1} \\
  &= B - \sum_{k=1}^2
  \rho_k \Bigg( \frac{\partial \varphi_k(tP^*_{\# k})}{\partial t} \bigg|_{t=1} - \varphi_k(P^*_{\# k}) \Bigg)
  - \varepsilon \Bigg( \frac{\partial \varphi(tP^*)}{\partial t} \bigg|_{t=1} - \varphi(P^*) \Bigg) \\
  &= \sum_{k=1}^2 \rho_k G_{\varphi_k}(\mu_k) + \varepsilon G_{\varphi}(\gamma)
- \left[ \sum_{k=1}^2 \rho_k G_{\varphi_k}(P^*_{\# k}) - \varepsilon G_{\varphi}(P^*) \right],
\end{align}
since $\frac{\partial f(tx)}{ \partial t} \Big |_{t=1} = \langle \nabla f(x), x \rangle$,
for any $f \in \cC^1(\bbR^d)$. The result then follows.
\end{proof}

\begin{corollary}
  \label{coro:conseq}
  Equations \eqref{uot_l2}, \eqref{eq:ugw_minimizer_minimum} and \eqref{eq:ucoot_minimizer_minimum}
  holds.
\end{corollary}
\begin{proof}[Proof of \Cref{coro:conseq}]
  Equation \eqref{uot_l2} on the squared $l_2$-regularized UOT follows immediately from \Cref{prop:uot_minimizer},
  where $D_{\varphi}, D_{\varphi_1}$ and $D_{\varphi_2}$ are the half squared Euclidean norm,
  and $E = \bbR^{m \times n}_{\geq 0}$.

  Equation \eqref{eq:ugw_minimizer_minimum} on unbalanced Gromov-Wasserstein (UGW) and
  Equation \eqref{eq:ucoot_minimizer_minimum} on unbalanced Co-Optimal Transport can be
  justified in exactly the same manner. For this reason, we only show how to derive the relation in
  Equation \eqref{eq:ucoot_minimizer_minimum}. When $D_{\varphi}, D_{\varphi_1}$ and $D_{\varphi_2}$
  are the KL divergence, observe that for any $t > 0$, we have
  \begin{align}
    \kl\big( (tp) \otimes (tp) | q \otimes q \big) &= \kl\big( t^2 (p \otimes p) | q \otimes q \big) \\
    &= t^2 \kl\big( p \otimes p | q \otimes q \big) + (1 - t^2) m(q)^2 + 2 t^2 \log t \; m(p)^2.
  \end{align}
  Now we proceed in the same way as the proof of \Cref{prop:uot_minimizer}. Denote $F$ the objective function
  of UGW, then
  \begin{align}
    F(t\pi) &= t^2 F(\pi) + (1 - t^2) \underbrace{\big[\lambda_1 m(\mu_X)^2 + \lambda_2 m(\mu_Y)^2 \big]}_{M}
    + 2 \underbrace{(\lambda_1 + \lambda_2)}_{\lambda} t^2 \log t \; m(\pi)^2 \\
    &= t^2 F(\pi) + (1 - t^2) M + 2 \lambda t^2 \log t \; m(\pi)^2.
  \end{align}
  We have
  \begin{align}
    \frac{\partial F(t\pi)}{\partial t} &= 2t F(\pi) - 2t M + 2 \lambda m(\pi)^2 (t + 2t \log t) \\
    &= 2t \big[ F(\pi) - M + \lambda m(\pi)^2 + 2 \lambda m(\pi)^2 \log t \big].
  \end{align}
  If $\pi^*$ is a minimizer, then $\frac{\partial F(t\pi^*)}{\partial t} \Big |_{t = 1} = 0$.
  We deduce that $F(\pi^*) - M + \lambda m(\pi^*)^2 = 0$. The result then follows.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proofs related to Gromov-Wasserstein distance} \label{subsec:appendix_gw}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{corollary}
  \label{coro:gh_equiv}
    The formulations \eqref{GH:zeroth} and \eqref{GH:first} of the Gromov-Hausdorff distance
    are equivalent.
\end{corollary}
\begin{proof}[Proof of \Cref{coro:gh_equiv}]
In the formulation \eqref{GH:first}, by choosing identity mappings (which are clearly isometric embeddings)
$f = \id_X$ and $g = \id_Y$, and $Z = X \cup Y$ equipped with an admissible distance $d$, we have
\begin{equation}
  \gh((X,d_X), (Y,d_Y)) \leq d_{H}^{(Z,d)}(X, Y)
\end{equation}
As this is true for any $d \in \cD(d_X,d_Y)$, we have
$\gh((X,d_X), (Y,d_Y)) \leq \inf_{d} d_{H}^{(X \cup Y, d)}(X, Y)$. For the reverse direction,
given any isometries $f: X \to X'$ and $g: Y \to Y'$ with $X', Y' \subset (Z, d_Z)$
(we call $(X',Y')$ a \textit{metric coupling} \citep{Villani08}), one can define
a distance $d$ on $X \cup Y$ (as shown in Proposition 27.1 in \citep{Villani08}). Then,
\begin{equation}
  \begin{split}
    d_H^{(Z, d_Z)}(f(X), g(Y)) &=
  \max(\sup_{y \in Y} d_Z(g(y), f(X)), \sup_{x \in X} d_Z(f(x), g(Y))) \\
  &= \max(\sup_{y \in Y} d(y, X), \sup_{x \in X} d(x, Y)) \\
  &= d_H^{(X \cup Y, d)}(X, Y) \geq \inf_{d'} d_{H}^{(X \cup Y, d')}(X, Y)
  \end{split}
\end{equation}
We deduce that $\gh((X,d_X), (Y,d_Y)) \geq \inf_{d} d_{H}^{(X \cup Y, d)}(X, Y)$,
thus equality holds.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}[Proof of \Cref{lemma:fgw_pgd}]
  Denote $F(P) = \langle C \otimes P, P \rangle + \langle M, P \rangle + \varepsilon \kl(P | \gamma)$.
  Then $\nabla F(P) = C \otimes P + M + \varepsilon \log \frac{P}{\gamma}$.
  The PGD iterate reads: for $\tau > 0$,
  \begin{align}
    P^{(t+1)} = \text{proj}^{\kl}_{U(\mu_x, \mu_y)}
    \left( P^{(t)} \odot e^{-\tau \nabla F(P^{(t)})} \right),
  \end{align}
  where $\text{proj}^{\kl}_{U(\mu_x, \mu_y)}(K) =
  \argmin_{P \in U(\mu_x, \mu_y)} - \varepsilon \langle \log K, P \rangle + \varepsilon H(P)$.
  Denote $\eta = \tau \varepsilon$. We have
  \begin{align}
    \log K &= \log \left( P^{(t)} \odot e^{-\tau \nabla F(P^{(t)})} \right) \\
    &= \log P^{(t)} - \tau (C \otimes P^{(t)} + M)  - \tau \varepsilon \log \frac{P^{(t)}}{\gamma} \\
    &= - \tau (C \otimes P^{(t)} + M) + \log \left( \gamma^{\eta} \odot (P^{(t)})^{1 - \eta} \right).
  \end{align}
  We deduce that
  \begin{align}
    &- \varepsilon \langle \log K, P \rangle + \varepsilon H(P) \\
    &= \eta \langle C \otimes P^{(t)} + M, P \rangle
    - \varepsilon \langle P, \log \left( \gamma^{\eta} \odot (P^{(t)})^{1 - \eta} \right) \rangle
    + \varepsilon H(P) \\
    &= \eta \langle C \otimes P^{(t)} + M, P \rangle +
    \varepsilon \kl \left( P \big | \gamma^{\eta} \odot (P^{(t)})^{1 - \eta} \right)
    - m \left( \gamma^{\eta} \odot (P^{(t)})^{1 - \eta} \right),
  \end{align}
  where $m(Q) = \sum_{i,j} Q_{ij}$ is the mass of measure $Q$. The result then follows.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%