\section{Appendix of Chapter 2}

\subsection{Proofs related to Unbalanced Optimal Transport} \label{appendix:subsec_uot}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}[Proof of \Cref{coro:uot_l2_mm}]
Following \citep{Chapel21}, we can write Problem \eqref{uot_l2} as
\begin{align}
    \min_{t \in \bbR^{mn}_{\geq 0}} \langle c, t \rangle + \frac{|| Mt - y ||_2^2}{2},
\end{align}
where $t = \vect(P), c = \vect(C)$ are the vectorizations of $P, C$, respectively. Here,
\begin{itemize}
    \item[$\bullet$] $M = (\rho_1^{1/2} M_r^T, \rho_2^{1/2} M_c^T, \varepsilon^{1/2} I_{mn})^T \in \bbR^{(m + n + mn) \times (mn)}$.

    \item[$\bullet$] $y = (\rho_1^{1/2} \mu^T, \rho_2^{1/2} \nu^T, \varepsilon^{1/2} \vect(\gamma)^T)^T \in \bbR^{m + n + mn}$.

    \item[$\bullet$] $M_r = \texttt{numpy.repeat(numpy.eye(n),m)} \in \bbR^{n \times mn}$.

    \item[$\bullet$] $M_c = [I_m, ..., I_m] = \texttt{numpy.tile(numpy.eye(m),n)} \in \bbR^{m \times mn}$.
\end{itemize}
See Appendix A in \citep{Chapel21} for more details of $M_r, M_c$.
Remark that if $A \in \bbR^{m \times n}$ and $B \in \bbR^{m \times p}$, then
\begin{align}
    (A, B) \begin{pmatrix}
        A^T \\
        B^T
    \end{pmatrix}
    = A A^T + B B^T.
\end{align}
Following Equation 23 in \citep{Chapel21}, for $i \in [mn]$,
\begin{align}
    t^{(k+1)}_i = t^{(k)}_i \frac{\max \big\{ 0, (M^T y)_i - c_i \big\}}{(M^T M t^{(k)})_i}.
\end{align}
Now, we convert the vector $t$ back to the transport plan. More precisely,
$\text{mat}(M^T y) = (\rho_1 \mu) \oplus (\rho_2 \nu) + \varepsilon \gamma$,
where matrization is the inverse operation of vectorization. Since,
$M^T M = \rho_1 M_r^T M_r + \rho_2 M_c^T M_c + \varepsilon I_{mn}$, we obtain
$\text{mat}(M^T M t^{(k)}) = (\rho_1 P_{\# 1}^{(k)}) \oplus (\rho_2 P_{\# 2}^{(k)})
+ \varepsilon P^{(k)} $. The result then follows.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}[Proof of \Cref{prop:uot_minimizer}]
We follow the same proof technique of Lemma 4 in \citep{Khiem20}.
Given a Bregman divergence $D_{\psi}$, for any $t \in \bbR$ such that $tp \in \dom(\psi)$,
we have
\begin{align}
  D_{\psi}(t p | q)
  &= \psi(tp) - \psi(q) - \langle \nabla \psi(q), tp - q \rangle \\
  &= \psi(tp) - \psi(q) - \Big[
    t \langle \nabla \psi(q), p - q \rangle + (t-1) \langle \nabla \psi(q), q \rangle
  \Big] \\
  &= \psi(tp) - \psi(q) + t \Big[ D_{\psi}(p | q) + \psi(q) - \psi(p) \Big]
  - (t-1) \langle \nabla \psi(q), q \rangle \\
  &= t D_{\psi}(p | q) + \big[ \psi(tp) - t \psi(p) \big]
  + (t-1) \big[ \psi(q) - \langle \nabla \psi(q), q \rangle \big] \\
  &= t D_{\psi}(p | q) + \big[ \psi(tp) - t \psi(p) \big] + (1 - t) G_{\psi}(q).
\end{align}
Denote $g$ the objective function of Problem \eqref{eq:uot_bregman}. We have,
\begin{align}
  g(tP) &= t g(P) +
  \underbrace{\Big( \sum_{k=1}^2 \rho_k \big[ \varphi_k(tP_{\# k}) - t \varphi_k(P_{\# k}) \big]
  + \varepsilon \big[ \varphi(tP) - t \varphi(P) \big] \Big)}_{A(t)} \\
  &+ (1 - t) \underbrace{\big[ \rho_k G_{\varphi_k}(\mu_k) + \varepsilon G_{\varphi}(\gamma) \big]}_{B} \\
    &= t g(P) + A(t) + (1 - t) B.
\end{align}
Since $P^*$ is a minimizer, we must have $1 \in \argmin\limits_{t \in \bbR: tP^* \in \cD} g(tP^*)$
or equivalently $\frac{\partial g(tP^*)}{\partial t} \Big |_{t = 1} = 0$. So,
\begin{align}
  g(P^*) &= B - \frac{\partial A(t)}{\partial t} \bigg|_{t=1} \\
  &= B - \sum_{k=1}^2
  \rho_k \Bigg( \frac{\partial \varphi_k(tP^*_{\# k})}{\partial t} \bigg|_{t=1} - \varphi_k(P^*_{\# k}) \Bigg)
  - \varepsilon \Bigg( \frac{\partial \varphi(tP^*)}{\partial t} \bigg|_{t=1} - \varphi(P^*) \Bigg) \\
  &= \sum_{k=1}^2 \rho_k G_{\varphi_k}(\mu_k) + \varepsilon G_{\varphi}(\gamma)
- \left[ \sum_{k=1}^2 \rho_k G_{\varphi_k}(P^*_{\# k}) - \varepsilon G_{\varphi}(P^*) \right],
\end{align}
since $\frac{\partial f(tx)}{ \partial t} \Big |_{t=1} = \langle \nabla f(x), x \rangle$,
for any $f \in \cC^1(\bbR^d)$. The result then follows.
\end{proof}

\begin{corollary}
  \label{coro:conseq}
  Equations \eqref{uot_l2}, \eqref{eq:ugw_minimizer_minimum} and \eqref{eq:ucoot_minimizer_minimum}
  are consequences of \Cref{prop:uot_minimizer}.
\end{corollary}
\begin{proof}[Proof of \Cref{coro:conseq}]
  Equation \eqref{uot_l2} on the squared $l_2$-regularized UOT follows immediately from \Cref{prop:uot_minimizer},
  where $D_{\varphi}, D_{\varphi_1}$ and $D_{\varphi_2}$ are the half squared Euclidean norm,
  and $E = \bbR^{m \times n}_{\geq 0}$. When
  $D_{\varphi}, D_{\varphi_1}$ and $D_{\varphi_2}$ are the KL divergence,
  Equation \eqref{eq:ugw_minimizer_minimum} on unbalanced Gromov-Wasserstein and
  Equation \eqref{eq:ucoot_minimizer_minimum} on unbalanced Co-Optimal Transport can be
  justified in exactly the same manner. For this reason, we only show how to derive the relation in
  Equation \eqref{eq:ucoot_minimizer_minimum}. Recall that,
  from the proof of \Cref{eq:ucoot_existence_copy}, Problem \eqref{eq:ucoot_copy} can be rewritten as
    \begin{equation}
      \label{eq:ucoot_uot}
      \ucoot_{\rho}(\cX_1, \cX_2) =
      \inf_{\pi \in E_{uco}} \int_{S} \vert \xi_1 - \xi_2 \vert^p
      \mathrm d\pi + \sum_{k=1,2} \rho_k D_{\phi_k}(\pi_{\# k} \vert \mu_k),
    \end{equation}
    where $\mu_k = \mu_k^s \otimes \mu_k^f$, for $k=1,2$, and
    \begin{equation}
      E_{uco} = \{ \pi \in \cM^+(S) \vert \pi = \pi^s \otimes \pi^f,
      \pi^s \in \cM^+(X_1^s \times X_2^s),
      \pi^f \in \cM^+(X_1^f \times X_2^f) \}
    \end{equation}
    is the set of factorizable plans in $\cM^+(S)$. Now, clearly, for any $t > 0$,
    if $\pi \in E_{uco}$, then $t \pi \in E_{uco}$. So, \Cref{prop:uot_minimizer} can be applied
    and we deduce that, if $\pi_* = \pi_*^s \otimes \pi_*^f$ is the solution of Problem
    \eqref{eq:ucoot_uot}, then
    \begin{align}
      \ucoot_{\rho}(\cX_1, \cX_2) &= \sum_{k=1}^2 \rho_k m(\mu_k) - (\rho_1 + \rho_2) m(\pi_*) \\
      &= \sum_{k=1}^2 \rho_k m(\mu_k^s) m(\mu_k^f) - (\rho_1 + \rho_2) m(\pi_*^s) m(\pi_*^f).
    \end{align}
    The result then follows.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proofs related to Gromov-Wasserstein distance} \label{subsec:appendix_gw}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{corollary}
  \label{coro:gh_equiv}
    The formulations \eqref{GH:zeroth} and \eqref{GH:first} of the Gromov-Hausdorff distance
    are equivalent.
\end{corollary}
\begin{proof}[Proof of \Cref{coro:gh_equiv}]
In the formulation \eqref{GH:first}, by choosing identity mappings (which are clearly isometric embeddings)
$f = \id_X$ and $g = \id_Y$, and $Z = X \cup Y$ equipped with an admissible distance $d$, we have
\begin{equation}
  \gh((X,d_X), (Y,d_Y)) \leq d_{H}^{(Z,d)}(X, Y)
\end{equation}
As this is true for any $d \in \cD(d_X,d_Y)$, we have
$\gh((X,d_X), (Y,d_Y)) \leq \inf_{d} d_{H}^{(X \cup Y, d)}(X, Y)$. For the reverse direction,
given any isometries $f: X \to X'$ and $g: Y \to Y'$ with $X', Y' \subset (Z, d_Z)$
(we call $(X',Y')$ a \textit{metric coupling} \citep{Villani08}), one can define
a distance $d$ on $X \cup Y$ (as shown in Proposition 27.1 in \citep{Villani08}). Then,
\begin{equation}
  \begin{split}
    d_H^{(Z, d_Z)}(f(X), g(Y)) &=
  \max(\sup_{y \in Y} d_Z(g(y), f(X)), \sup_{x \in X} d_Z(f(x), g(Y))) \\
  &= \max(\sup_{y \in Y} d(y, X), \sup_{x \in X} d(x, Y)) \\
  &= d_H^{(X \cup Y, d)}(X, Y) \geq \inf_{d'} d_{H}^{(X \cup Y, d')}(X, Y)
  \end{split}
\end{equation}
We deduce that $\gh((X,d_X), (Y,d_Y)) \geq \inf_{d} d_{H}^{(X \cup Y, d)}(X, Y)$,
thus equality holds.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}[Proof of \Cref{lemma:fgw_pgd}]
  Denote $F(P) = \langle C \otimes P, P \rangle + \langle M, P \rangle + \varepsilon \kl(P | \gamma)$.
  Then $\nabla F(P) = C \otimes P + M + \varepsilon \log \frac{P}{\gamma}$.
  The PGD iterate reads: for $\tau > 0$,
  \begin{align}
    P^{(t+1)} = \text{proj}^{\kl}_{U(\mu_x, \mu_y)}
    \left( P^{(t)} \odot e^{-\tau \nabla F(P^{(t)})} \right),
  \end{align}
  where $\text{proj}^{\kl}_{U(\mu_x, \mu_y)}(K) =
  \argmin_{P \in U(\mu_x, \mu_y)} - \varepsilon \langle \log K, P \rangle + \varepsilon H(P)$.
  Denote $\eta = \tau \varepsilon$. We have
  \begin{align}
    \log K &= \log \left( P^{(t)} \odot e^{-\tau \nabla F(P^{(t)})} \right) \\
    &= \log P^{(t)} - \tau (C \otimes P^{(t)} + M)  - \tau \varepsilon \log \frac{P^{(t)}}{\gamma} \\
    &= - \tau (C \otimes P^{(t)} + M) + \log \left( \gamma^{\eta} \odot (P^{(t)})^{1 - \eta} \right).
  \end{align}
  We deduce that
  \begin{align}
    &- \varepsilon \langle \log K, P \rangle + \varepsilon H(P) \\
    &= \eta \langle C \otimes P^{(t)} + M, P \rangle
    - \varepsilon \langle P, \log \left( \gamma^{\eta} \odot (P^{(t)})^{1 - \eta} \right) \rangle
    + \varepsilon H(P) \\
    &= \eta \langle C \otimes P^{(t)} + M, P \rangle +
    \varepsilon \kl \left( P \big | \gamma^{\eta} \odot (P^{(t)})^{1 - \eta} \right)
    - m \left( \gamma^{\eta} \odot (P^{(t)})^{1 - \eta} \right),
  \end{align}
  where $m(Q) = \sum_{i,j} Q_{ij}$ is the mass of measure $Q$. The result then follows.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%